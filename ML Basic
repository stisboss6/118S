{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stisboss6/118S/blob/main/ML%20Basic\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1: Predict house prices based on square footage and location\n",
        "# --------------------------------------------------------------\n",
        "# This script:\n",
        "# 1) Loads a dataset from CSV (square footage, location, price)\n",
        "# 2) One-hot encodes the categorical 'location' feature\n",
        "# 3) Trains a Linear Regression model\n",
        "# 4) Predicts the price of a 2000 sq ft house in Downtown\n",
        "# 5) Prints model coefficients so you can see feature impacts\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "CSV_PATH = \"part1_housing_3location_dataset.csv\"\n",
        "\n",
        "# 1) Load dataset from CSV\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# (Optional) quick check\n",
        "# print(df.head())\n",
        "# print(df.columns)\n",
        "\n",
        "# 2) Features and target\n",
        "X = df[[\"square_footage\", \"location\"]]\n",
        "y = df[\"price\"]\n",
        "\n",
        "# 3) Preprocessing: one-hot encode 'location', pass through square_footage as-is\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"location\", OneHotEncoder(handle_unknown=\"ignore\"), [\"location\"]),\n",
        "    ],\n",
        "    remainder=\"passthrough\",\n",
        ")\n",
        "\n",
        "# 4) Pipeline: preprocessing + linear regression model\n",
        "model = Pipeline(\n",
        "    steps=[\n",
        "        (\"preprocessor\", preprocessor),\n",
        "        (\"regressor\", LinearRegression()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 5) Split and train\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 6) Predict for a new house (2000 sq ft in Downtown)\n",
        "new_house = pd.DataFrame({\"square_footage\": [2000], \"location\": [\"Downtown\"]})\n",
        "predicted_price = model.predict(new_house)[0]\n",
        "print(f\"Predicted price for a 2000 sq ft house in Downtown: ${predicted_price:,.2f}\")\n",
        "\n",
        "# 7) Print coefficients (impact of each feature)\n",
        "# Get feature names in the same order used by the preprocessor output\n",
        "ohe = model.named_steps[\"preprocessor\"].named_transformers_[\"location\"]\n",
        "location_feature_names = ohe.get_feature_names_out([\"location\"]).tolist()\n",
        "\n",
        "# remainder='passthrough' appends square_footage after one-hot columns\n",
        "feature_names = location_feature_names + [\"square_footage\"]\n",
        "\n",
        "coefficients = model.named_steps[\"regressor\"].coef_\n",
        "\n",
        "print(\"\\nModel Coefficients (higher means higher price, holding others constant):\")\n",
        "for feature, coef in zip(feature_names, coefficients):\n",
        "    print(f\"{feature:>20}: {coef:,.2f}\")\n",
        "\n",
        "print(f\"\\nIntercept: {model.named_steps['regressor'].intercept_:,.2f}\")\n"
      ],
      "metadata": {
        "id": "wbOE0tDs-7IR",
        "outputId": "2c446d80-1e9a-4140-bcf8-1faaa5e872ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted price for a 2000 sq ft house in Downtown: $173,199.46\n",
            "\n",
            "Model Coefficients (higher means higher price, holding others constant):\n",
            "   location_Downtown: -36,400.26\n",
            "      location_Rural: 32,363.21\n",
            "     location_Suburb: 4,037.05\n",
            "      square_footage: 20.38\n",
            "\n",
            "Intercept: 168,835.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Part 1 (using generated CSV): Predict house price from square footage + location\n",
        "\n",
        "Dataset file (generated from housing.csv):\n",
        "    part1_housing_3location_dataset.csv\n",
        "Columns:\n",
        "    - square_footage (numeric)\n",
        "    - location (categorical: Downtown, Suburb, Rural)\n",
        "    - price (numeric target)\n",
        "\n",
        "This matches the assignment structure:\n",
        "- One-hot encode location\n",
        "- Train Linear Regression\n",
        "- Predict for a new house (2000 sq ft in Downtown)\n",
        "- Print coefficients\n",
        "\n",
        "Run:\n",
        "    python part1_using_generated_csv.py\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "CSV_PATH = \"part1_housing_3location_dataset.csv\"\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    # 1) Load dataset\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "    # 2) Features + target\n",
        "    X = df[[\"square_footage\", \"location\"]]\n",
        "    y = df[\"price\"]\n",
        "\n",
        "    # 3) Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # 4) Preprocessing (like Part 2 style)\n",
        "    numeric_features = [\"square_footage\"]\n",
        "    categorical_features = [\"location\"]\n",
        "\n",
        "    numeric_transformer = Pipeline(\n",
        "        steps=[\n",
        "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "            (\"scaler\", StandardScaler()),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    categorical_transformer = Pipeline(\n",
        "        steps=[\n",
        "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", numeric_transformer, numeric_features),\n",
        "            (\"cat\", categorical_transformer, categorical_features),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # 5) Model pipeline\n",
        "    model = Pipeline(\n",
        "        steps=[\n",
        "            (\"preprocessor\", preprocessor),\n",
        "            (\"regressor\", LinearRegression()),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # 6) Train\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 7) Evaluate\n",
        "    preds = model.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, preds)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
        "    r2 = r2_score(y_test, preds)\n",
        "\n",
        "    print(\"=== Part 1: House Price Prediction (Linear Regression) ===\")\n",
        "    print(\"\\n--- Test Set Performance ---\")\n",
        "    print(f\"MAE : ${mae:,.0f}\")\n",
        "    print(f\"RMSE: ${rmse:,.0f}\")\n",
        "    print(f\"R^2 : {r2:.3f}\")\n",
        "\n",
        "    # 8) Predict a new house (edit these values if your assignment uses different inputs)\n",
        "    new_house = pd.DataFrame({\"square_footage\": [2000], \"location\": [\"Downtown\"]})\n",
        "    predicted_price = model.predict(new_house)[0]\n",
        "    print(\"\\n--- Example Prediction ---\")\n",
        "    print(f\"Predicted price for a 2000 sq ft house in Downtown: ${predicted_price:,.2f}\")\n",
        "\n",
        "    # 9) Print coefficients (feature impact)\n",
        "    # Get feature names in the same order as the model sees them\n",
        "    ohe = model.named_steps[\"preprocessor\"].named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
        "    cat_names = ohe.get_feature_names_out([\"location\"]).tolist()\n",
        "    feature_names = [\"square_footage (scaled)\"] + cat_names\n",
        "\n",
        "    coefs = model.named_steps[\"regressor\"].coef_\n",
        "    intercept = model.named_steps[\"regressor\"].intercept_\n",
        "\n",
        "    print(\"\\n--- Model Coefficients ---\")\n",
        "    print(\"Note: square_footage is scaled (1 unit = 1 standard deviation).\")\n",
        "    for name, coef in zip(feature_names, coefs):\n",
        "        print(f\"{name:>25}: {coef:,.2f}\")\n",
        "    print(f\"{'Intercept':>25}: {intercept:,.2f}\")\n",
        "\n",
        "    # Optional: save a test-set predictions file\n",
        "    out = X_test.copy()\n",
        "    out[\"actual_price\"] = y_test.values\n",
        "    out[\"predicted_price\"] = preds\n",
        "    out.to_csv(\"part1_predictions.csv\", index=False)\n",
        "    print(\"\\nSaved: part1_predictions.csv (test-set actual vs predicted)\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "yR0WQP0F9Q0f",
        "outputId": "b265faa2-80fa-4647-c2a8-a3709cadb96d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Part 1: House Price Prediction (Linear Regression) ===\n",
            "\n",
            "--- Test Set Performance ---\n",
            "MAE : $85,301\n",
            "RMSE: $110,034\n",
            "R^2 : 0.076\n",
            "\n",
            "--- Example Prediction ---\n",
            "Predicted price for a 2000 sq ft house in Downtown: $173,199.46\n",
            "\n",
            "--- Model Coefficients ---\n",
            "Note: square_footage is scaled (1 unit = 1 standard deviation).\n",
            "  square_footage (scaled): 17,201.39\n",
            "        location_Downtown: -36,400.26\n",
            "           location_Rural: 32,363.21\n",
            "          location_Suburb: 4,037.05\n",
            "                Intercept: 207,616.70\n",
            "\n",
            "Saved: part1_predictions.csv (test-set actual vs predicted)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Part 2: Predict Customer Churn (Logistic Regression)\n",
        "\n",
        "This script follows the assignment steps:\n",
        "1) Create a sample dataset with customer features and churn status.\n",
        "2) Use StandardScaler for numerical features and OneHotEncoder for categorical 'region'.\n",
        "3) Train a LogisticRegression model using scikit-learn.\n",
        "4) Predict churn probability for a new customer and classify using a 0.5 threshold.\n",
        "5) Print model coefficients to show the impact of each feature on churn likelihood.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 1) Generate sample customer data\n",
        "data = {\n",
        "    \"age\": [25, 34, 45, 28, 52, 36, 41, 29, 47, 33],\n",
        "    \"monthly_usage_hours\": [10, 50, 20, 15, 60, 30, 25, 12, 55, 40],\n",
        "    \"purchase_amount\": [100, 250, 150, 80, 300, 200, 175, 90, 280, 220],\n",
        "    \"customer_service_calls\": [5, 2, 8, 6, 1, 3, 7, 4, 0, 2],\n",
        "    \"region\": [\"North\", \"South\", \"West\", \"East\", \"South\", \"North\", \"West\", \"East\", \"South\", \"North\"],\n",
        "    \"churn\": [1, 0, 1, 1, 0, 0, 1, 1, 0, 0],  # 1 = churned, 0 = not churned\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 2) Features and target\n",
        "X = df[[\"age\", \"monthly_usage_hours\", \"purchase_amount\", \"customer_service_calls\", \"region\"]]\n",
        "y = df[\"churn\"]\n",
        "\n",
        "# 3) Preprocessing: scale numerical features and one-hot encode categorical features\n",
        "num_features = [\"age\", \"monthly_usage_hours\", \"purchase_amount\", \"customer_service_calls\"]\n",
        "cat_features = [\"region\"]\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), num_features),\n",
        "        (\"cat\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), cat_features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 4) Pipeline: preprocessing + logistic regression model\n",
        "model = Pipeline(\n",
        "    steps=[\n",
        "        (\"preprocessor\", preprocessor),\n",
        "        (\"classifier\", LogisticRegression(random_state=42, max_iter=1000)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 5) Split data and train\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Optional: evaluate quickly on the test set (not required, but useful)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Test Accuracy:\", round(accuracy_score(y_test, y_pred), 3))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "# 6) Predict churn probability for a new customer and classify with a threshold\n",
        "new_customer = pd.DataFrame(\n",
        "    {\n",
        "        \"age\": [35],\n",
        "        \"monthly_usage_hours\": [20],\n",
        "        \"purchase_amount\": [150],\n",
        "        \"customer_service_calls\": [5],\n",
        "        \"region\": [\"West\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "churn_probability = model.predict_proba(new_customer)[0][1]  # Probability of churn (class 1)\n",
        "threshold = 0.5\n",
        "churn_prediction = 1 if churn_probability > threshold else 0\n",
        "\n",
        "print(f\"\\nChurn Probability for new customer: {churn_probability:.2f}\")\n",
        "print(f\"Churn Prediction (1 = churn, 0 = no churn): {churn_prediction} (threshold={threshold})\")\n",
        "\n",
        "# 7) Display model coefficients (feature impacts)\n",
        "# Prefer ColumnTransformer.get_feature_names_out when available\n",
        "try:\n",
        "    feature_names = model.named_steps[\"preprocessor\"].get_feature_names_out().tolist()\n",
        "except Exception:\n",
        "    # Fallback: build names manually\n",
        "    ohe = model.named_steps[\"preprocessor\"].named_transformers_[\"cat\"]\n",
        "    cat_names = ohe.get_feature_names_out(cat_features).tolist()\n",
        "    feature_names = num_features + cat_names\n",
        "\n",
        "coefficients = model.named_steps[\"classifier\"].coef_[0]\n",
        "intercept = model.named_steps[\"classifier\"].intercept_[0]\n",
        "\n",
        "print(\"\\nModel Coefficients (positive => higher churn likelihood, negative => lower):\")\n",
        "for name, coef in sorted(zip(feature_names, coefficients), key=lambda x: abs(x[1]), reverse=True):\n",
        "    print(f\"{name:>30}: {coef:+.4f}\")\n",
        "print(f\"\\nIntercept: {intercept:+.4f}\")\n"
      ],
      "metadata": {
        "id": "26vhslKgu_Ls",
        "outputId": "dff00f09-512a-4859-c1e2-3aed2417e930",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 1.0\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1 0]\n",
            " [0 1]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0      1.000     1.000     1.000         1\n",
            "           1      1.000     1.000     1.000         1\n",
            "\n",
            "    accuracy                          1.000         2\n",
            "   macro avg      1.000     1.000     1.000         2\n",
            "weighted avg      1.000     1.000     1.000         2\n",
            "\n",
            "\n",
            "Churn Probability for new customer: 0.77\n",
            "Churn Prediction (1 = churn, 0 = no churn): 1 (threshold=0.5)\n",
            "\n",
            "Model Coefficients (positive => higher churn likelihood, negative => lower):\n",
            "   num__customer_service_calls: +0.8659\n",
            "          num__purchase_amount: -0.6679\n",
            "      num__monthly_usage_hours: -0.6363\n",
            "              cat__region_West: +0.2852\n",
            "             cat__region_North: -0.2636\n",
            "             cat__region_South: -0.1108\n",
            "              cat__region_East: +0.0901\n",
            "                      num__age: -0.0604\n",
            "\n",
            "Intercept: -0.0987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Part 3: Customer Segmentation (K-Means clustering)\n",
        "\n",
        "Assignment checklist:\n",
        "1) Create a sample dataset with customer features: annual_spending, purchase_frequency, age, region.\n",
        "2) Scale numerical features (annual_spending, purchase_frequency, age) using StandardScaler.\n",
        "3) Use the elbow method to determine optimal number of clusters by plotting inertia for K=1..5.\n",
        "4) Apply K-Means clustering with K=3.\n",
        "5) Analyze mean characteristics of each cluster and suggest targeted marketing strategies.\n",
        "6) Save cluster assignments to a CSV file and generate an elbow plot (elbow_plot.png).\n",
        "\n",
        "Outputs created in the current working directory:\n",
        "- elbow_plot.png\n",
        "- customer_segments.csv\n",
        "\"\"\"\n",
        "\n",
        "import warnings\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    # 1) Generate sample customer data\n",
        "    data = {\n",
        "        \"annual_spending\": [500, 1200, 300, 1500, 800, 200, 1000, 600, 1300, 400],\n",
        "        \"purchase_frequency\": [5, 12, 3, 15, 8, 2, 10, 6, 13, 4],\n",
        "        \"age\": [25, 34, 45, 28, 52, 36, 41, 29, 47, 33],\n",
        "        \"region\": [\"North\", \"South\", \"West\", \"East\", \"South\", \"North\", \"West\", \"East\", \"South\", \"North\"],\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # 2) Preprocess: select numerical features and scale them\n",
        "    features = [\"annual_spending\", \"purchase_frequency\", \"age\"]\n",
        "    X = df[features]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # 3) Elbow method (K=1..5)\n",
        "    inertia = []\n",
        "    k_values = range(1, 6)\n",
        "\n",
        "    # Newer sklearn warns if n_init not set; we explicitly set it.\n",
        "    # We also silence any non-critical convergence warnings for a tiny toy dataset.\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        for k in k_values:\n",
        "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "            kmeans.fit(X_scaled)\n",
        "            inertia.append(kmeans.inertia_)\n",
        "\n",
        "    # Save elbow plot\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(list(k_values), inertia, marker=\"o\")\n",
        "    plt.xlabel(\"Number of Clusters (K)\")\n",
        "    plt.ylabel(\"Inertia\")\n",
        "    plt.title(\"Elbow Method for Optimal K\")\n",
        "    plt.xticks(list(k_values))\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"elbow_plot.png\", dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Elbow method results (lower inertia is better, look for the 'elbow'):\")\n",
        "    for k, iner in zip(k_values, inertia):\n",
        "        print(f\"  K={k}: inertia={iner:.4f}\")\n",
        "\n",
        "    # 4) Apply K-Means with K=3 (typical elbow choice for this example)\n",
        "    optimal_k = 3\n",
        "    kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "    df[\"cluster\"] = kmeans_final.fit_predict(X_scaled)\n",
        "\n",
        "    # 5) Analyze clusters\n",
        "    cluster_summary = df.groupby(\"cluster\")[features].mean().round(2)\n",
        "    cluster_counts = df[\"cluster\"].value_counts().sort_index()\n",
        "\n",
        "    print(\"\\nCluster Characteristics (means):\")\n",
        "    print(cluster_summary)\n",
        "    print(\"\\nCustomers per cluster:\")\n",
        "    for c in range(optimal_k):\n",
        "        print(f\"  Cluster {c}: {cluster_counts.get(c, 0)} customers\")\n",
        "\n",
        "    # 5) Suggest strategies based on cluster means\n",
        "    print(\"\\nTargeted Strategies:\")\n",
        "    for c in range(optimal_k):\n",
        "        mean_spend = cluster_summary.loc[c, \"annual_spending\"]\n",
        "        mean_freq = cluster_summary.loc[c, \"purchase_frequency\"]\n",
        "        mean_age = cluster_summary.loc[c, \"age\"]\n",
        "\n",
        "        print(f\"\\nCluster {c} (avg spending=${mean_spend}, avg freq={mean_freq}, avg age={mean_age}):\")\n",
        "\n",
        "        # Simple, readable rule-based suggestions\n",
        "        if mean_spend >= 1100 and mean_freq >= 10:\n",
        "            print(\"- VIP segment: High spend + frequent purchases\")\n",
        "            print(\"  Strategy: Exclusive promotions, early access, premium loyalty tier.\")\n",
        "        elif mean_spend >= 1100:\n",
        "            print(\"- High spenders: Strong revenue contribution\")\n",
        "            print(\"  Strategy: Personalized high-value bundles, loyalty rewards, concierge support.\")\n",
        "        elif mean_freq >= 10:\n",
        "            print(\"- Frequent buyers: Consistent purchasing behavior\")\n",
        "            print(\"  Strategy: Subscription/auto-replenish options, bulk discounts, referral bonuses.\")\n",
        "        else:\n",
        "            print(\"- Lower engagement/value: Opportunity to grow\")\n",
        "            print(\"  Strategy: Re-engagement emails, first-time promos, product education, win-back offers.\")\n",
        "\n",
        "    # 6) Save cluster assignments to CSV\n",
        "    df.to_csv(\"customer_segments.csv\", index=False)\n",
        "    print(\"\\nSaved files:\")\n",
        "    print(\"- elbow_plot.png\")\n",
        "    print(\"- customer_segments.csv\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "0WuJXVzzxZ_d",
        "outputId": "27f7982c-42d4-4081-dfbd-a17b14c5946d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elbow method results (lower inertia is better, look for the 'elbow'):\n",
            "  K=1: inertia=30.0000\n",
            "  K=2: inertia=12.6646\n",
            "  K=3: inertia=7.2630\n",
            "  K=4: inertia=4.2419\n",
            "  K=5: inertia=2.9472\n",
            "\n",
            "Cluster Characteristics (means):\n",
            "         annual_spending  purchase_frequency    age\n",
            "cluster                                            \n",
            "0                1033.33               10.33  46.67\n",
            "1                 400.00                4.00  33.60\n",
            "2                1350.00               13.50  31.00\n",
            "\n",
            "Customers per cluster:\n",
            "  Cluster 0: 3 customers\n",
            "  Cluster 1: 5 customers\n",
            "  Cluster 2: 2 customers\n",
            "\n",
            "Targeted Strategies:\n",
            "\n",
            "Cluster 0 (avg spending=$1033.33, avg freq=10.33, avg age=46.67):\n",
            "- Frequent buyers: Consistent purchasing behavior\n",
            "  Strategy: Subscription/auto-replenish options, bulk discounts, referral bonuses.\n",
            "\n",
            "Cluster 1 (avg spending=$400.0, avg freq=4.0, avg age=33.6):\n",
            "- Lower engagement/value: Opportunity to grow\n",
            "  Strategy: Re-engagement emails, first-time promos, product education, win-back offers.\n",
            "\n",
            "Cluster 2 (avg spending=$1350.0, avg freq=13.5, avg age=31.0):\n",
            "- VIP segment: High spend + frequent purchases\n",
            "  Strategy: Exclusive promotions, early access, premium loyalty tier.\n",
            "\n",
            "Saved files:\n",
            "- elbow_plot.png\n",
            "- customer_segments.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}